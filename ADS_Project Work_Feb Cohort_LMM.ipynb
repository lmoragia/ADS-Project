{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Collect and Explore Data \n",
    "1.1. Download dataset from kaggle:https://www.kaggle.com/datasets/uciml/iris?resource=download\n",
    "1.2. Load the dataset using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset from CSV\n",
    "data = pd.read_csv('Iris.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Explore the dataset by checking its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Data Reprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Handle categorical variables e.g., 'Species':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the values in the 'species' column\n",
    "data['Species'] = data['Species'].replace({\n",
    "    'Iris-setosa': 1,\n",
    "    'Iris-versicolor': 2,\n",
    "    'Iris-virginica': 3\n",
    "})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "data.to_csv('cleaned_Iris_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data.drop('Species', axis=1)), columns=data.columns[:-1])\n",
    "data_scaled.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaled DataFrame to a new CSV file\n",
    "data_scaled.to_csv('data_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = data_scaled\n",
    "y = data['Species']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training Set Shapes:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"\\nTesting Set Shapes:\")\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Import classifiers and train models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Evaluate Models\n",
    "#Import classifiers and train models:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machines\": SVC(),\n",
    "    \"Decision Trees\": DecisionTreeClassifier(),\n",
    "    \"Random Forests\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Reshape X_train to a 2D array\n",
    "#X_train = X_train.reshape(1, -1)\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{name}: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Hyperparameter tuning using GridSearchCV (example for Random Forests):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 feature_selection using RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate the best model from Step 4 (e.g., Random Forests)\n",
    "best_model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Create the RFECV object and fit it to the training data\n",
    "selector = RFECV(best_model, step=1, cv=5, scoring='accuracy')\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features and their ranks\n",
    "selected_features = X_train.columns[selector.support_]\n",
    "feature_ranks = selector.ranking_\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "print(f\"Feature ranks: {feature_ranks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Remove target variable from the list of selected features if it's present and export in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert selected_features to a list\n",
    "selected_features_list = selected_features.tolist()\n",
    "\n",
    "# Remove target variable from the list of selected features if it's present\n",
    "#None\n",
    "\n",
    "# Create new dataframes with only the selected features\n",
    "X_train_selected = X_train[selected_features_list]\n",
    "X_test_selected = X_test[selected_features_list]\n",
    "\n",
    "# Save X_train_selected as a CSV file\n",
    "pd.DataFrame(X_train_selected).to_csv('X_train_selected.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Train the model with the list of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model using accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy with selected features: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate evaluation scores\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')  # Use macro, micro, or weighted\n",
    "recall = recall_score(y_test, y_pred, average='macro')        # Use macro, micro, or weighted\n",
    "f1 = f1_score(y_test, y_pred, average='macro')                # Use macro, micro, or weighted\n",
    "\n",
    "# For roc_auc_score, if it's multiclass, you need to use the `multi_class` parameter\n",
    "if len(set(y_test)) > 2:\n",
    "    auc_roc = roc_auc_score(y_test, best_model.predict_proba(X_test_selected), multi_class='ovr')  # or 'ovo'\n",
    "else:\n",
    "    auc_roc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation scores\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=0.5)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "\n",
    "# Save the confusion matrix as an image\n",
    "\n",
    "plt.savefig('confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 To save the best features list as json for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "selected_features_list = selected_features.tolist()\n",
    "\n",
    "with open(\"selected_features.json\", \"w\") as f:\n",
    "    json.dump(selected_features_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.7 To save the best model using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_model, \"best_model1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Create Streamlit Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model = joblib.load(\"best_model1.pkl\")\n",
    "\n",
    "# Define the app title and layout\n",
    "st.title(\"Iris Flower Species App\")\n",
    "\n",
    "# Define input fields for features\n",
    "id = st.number_input(\"Id\", min_value=0, max_value=100, value=60, step=1)\n",
    "sepal_length_cm = st.number_input(\"Sepal Length in CM\", min_value=0.0, max_value=10.0, value=5.0, step=0.1)\n",
    "sepal_width_cm = st.number_input(\"Sepal Width in CM\", min_value=0.0, max_value=10.0, value=5.0, step=0.1)\n",
    "petal_length_cm = st.number_input(\"Petal Length in CM\", min_value=0.0, max_value=10.0, value=5.0, step=0.1)\n",
    "petal_width_cm = st.number_input(\"Petal Width in CM\", min_value=0.0, max_value=10.0, value=5.0, step=0.1)\n",
    "\n",
    "# Create a button for making predictions\n",
    "if st.button(\"Predict\"):\n",
    "    # Process input values\n",
    "    input_data = pd.DataFrame(\n",
    "        {\n",
    "            \"Id\": [id],\n",
    "            \"SepalLengthCm\": [sepal_length_cm],\n",
    "            \"SepalWidthCm\": [sepal_width_cm],\n",
    "            \"PetalLengthCm\": [petal_length_cm],\n",
    "            \"PetalWidthCm\": [petal_width_cm]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Scale input data using the scaler used during training\n",
    "    input_data_scaled = scaler.transform(input_data)\n",
    "\n",
    "    # Make a prediction using the trained model\n",
    "    prediction = model.predict(input_data_scaled)\n",
    "\n",
    "    # Display the prediction\n",
    "    if prediction[0] == 1:\n",
    "        st.success(\"Unhealthy Flower.\")\n",
    "    else:\n",
    "        st.success(\"Healthy Flower.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
